In the parameter estimation problems, we obtain information about the parameter from a sample of data coming from the underlying probability distribution. 
A natural question is: how much information can a sample of data provide about the unknown parameter? 
% This section introduces such a measure for information, and we can also see that this information measure can be used to find bounds on the variance of estimators, and it can be used to approximate the sampling distribution of an estimator obtained from a large sample, and further be used to obtain an approximate confidence interval in case of large sample.
To this end, the FIM, which is the common tools for assessing the amount of available information in the sampled data is overviewed in the following.
\par
Consider a random variable $x$ for which the pdf is $f\rBrace{x|\theta}$, where $\theta$ is an unknown parameter and $\theta\in\Theta$, with $\Theta$ is the parameter space.
Intuitively, if an event has small probability, then the occurrence of this event brings us much information.
For a random variable $X \sim f\rBrace{x|\theta}$, if $\theta$ were the true value of the parameter, the likelihood function should take a big value, or equivalently, the derivative log-likelihood function should be close to zero, and this is the basic principle of maximum likelihood estimation.
We define $l\rBrace{x|\theta} = \log{f\rBrace{x|\theta}}$ as the log-likelihood function which follows that the Fisher information may be expressed \cite{van2004detection} as 
% We define $l\rBrace{x|\theta} = \log{f\rBrace{x|\theta}}$ as the log-likelihood function, and
% \begin{equation*}
%     l^{\prime}\rBrace{x|\theta} = \frac{\partial}{\partial\theta}\log{f\rBrace{x|\theta}}=\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}
% \end{equation*}
% where $f^{\prime}\rBrace{x|\theta}$ is the derivative of $f\rBrace{x|\theta}$ with respect to $\theta$.
% Similarly, we denote the second order derivative of $f\rBrace{x|\theta}$ with respect to $\theta$ as $f^{\prime\prime}\rBrace{x|\theta}$.
% According to the above analysis, if $l^{\prime}\rBrace{X|\theta}$ is close to zero, then it is expected, thus the random variable does not provide much information about $\theta$;
% on the other hand, if $\abs{l^{\prime}\rBrace{X|\theta}}$ or $\abs{l^{\prime}\rBrace{X|\theta}}^{2}$ is large, the random variable provides much information about $\theta$.
% Thus, we can use $\abs{l^{\prime}\rBrace{X|\theta}}^{2}$ to measure the amount of information provided by $X$.
% however, since $X$ is a random variable, we should consider the average case. Thus, we introduce the following definition:
% \begin{definition}
% \emph{Fisher information} (for $\theta$) contained in the random variable $X$ is defined as:
% \begin{equation}
% \label{eq_prlm_FI_def}
%     I\rBrace{\theta}=E_{\theta}\cBrace{\abs{l^{\prime}\rBrace{X|\theta}}^{2}}=\int{}\abs{l^{\prime}\rBrace{X|\theta}}^{2}f\rBrace{x|\theta}dx.
% \end{equation}
% \end{definition}
% We assume that we can exchange the order of differentiation and integration, then
% \begin{equation*}
%     \int{}f^{\prime}\rBrace{x|\theta}dx=\frac{\partial}{\partial\theta}\int{}f\rBrace{x|\theta}dx=0.
% \end{equation*}
% Similarly,
% \begin{equation*}
%     \int{}f^{\prime\prime}\rBrace{x|\theta}dx=\frac{\partial^{2}}{\partial\theta^{2}}\int{}f\rBrace{x|\theta}dx=0.
% \end{equation*}
% It is easy to see that
% \begin{equation*}
%     E_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}=\int{}l^{\prime}\rBrace{x|\theta}f\rBrace{x|\theta}dx=\int{}\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}f\rBrace{x|\theta}dx=\int{}f^{\prime}\rBrace{x|\theta}=0
% \end{equation*}
% Therefore, the definition of Fisher information \eqref{eq_prlm_FI_def} can be rewritten as
% \begin{equation}
% \label{eq_prlm_IeqVar}
%     I\rBrace{\theta}=\text{Var}_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}
% \end{equation}.
% Also, notice that
% \begin{equation*}
%     l^{\prime\prime}\rBrace{x|\theta}=\frac{\partial}{\partial\theta}\vBrace{\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}}=\frac{f^{\prime\prime}\rBrace{x|\theta}f\rBrace{x|\theta}-\abs{f^{\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}=\frac{\abs{f^{\prime\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}-\abs{l^{\prime}\rBrace{x|\theta}}^{2}.
% \end{equation*}
% Therefore,
% \begin{equation*}
%     E_{\theta}\cBrace{l^{\prime\prime}\rBrace{x|\theta}}=\int{\vBrace{\frac{\abs{f^{\prime\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}-\abs{l^{\prime}\rBrace{x|\theta}}^{2}}f\rBrace{x|\theta}}=\int{f^{\prime\prime}\rBrace{x|\theta}dx}-E_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}=-I\rBrace{\theta}.
% \end{equation*}
% Finally, we have another formula to calculate Fisher information:
\begin{equation}
\label{eq_prlm_Ifin}
    I\rBrace{\theta} = -E_{\theta}\cBrace{l^{\prime\prime}\rBrace{x|\theta}}=-\int{\vBrace{\frac{\partial^{2}}{\partial\theta^{2}}\log{f\rBrace{x|\theta}}f\rBrace{x|\theta}}dx}
\end{equation}.
% To summarize, we have three methods to calculate Fisher information: equations \eqref{eq_prlm_FI_def}, \eqref{eq_prlm_IeqVar} and \eqref{eq_prlm_Ifin}.
% In any problems, using \eqref{eq_prlm_Ifin} is the most convenient choice.
\subsection{Cram\'er-Rao Lower Bound and Asymptotic Distribution of Maximum Likelihood Estimators}
% Suppose that we have a random sample $X_{1},\dots,X_{n}$ coming from a distribution for which the pdf or pmf is $f\rBrace{x|\theta},$ where the value of the parameter $\theta$ is unknown. 
% We will show how to use Fisher information to determine the lower bound for the variance of an estimator of the parameter $\theta$.
% \par
% Let $\hat{\theta}=r\rBrace{X_{1},\dots,X_{n}}=r\rBrace{\vecnot{X}}$ be an arbitrary estimator of $\theta$.
% Assume $E_{\theta}\rBrace{\hat{\theta}}=m\rBrace{\theta}$ and the variance of $\hat{\theta}$ is finite.
% Let us consider the random variable $l^{\prime}\rBrace{\vecnot{X}|\theta}$, it can be shown that $E_{\theta}\cBrace{l^{\prime}\rBrace{\vecnot{X}|\theta}}=0$ and that
% \begin{equation}
% \label{eq_prlm_covThetaLTag}
%     \text{Cov}_{\theta}\cBrace{\hat{\theta},l^{\prime}\rBrace{\vecnot{X}|\theta}}=m^{\prime}\rBrace{\theta}.
% \end{equation}.
% By Cauchy-Schwartz inequality and the definition of $I_{n}\rBrace{\theta},$
% \begin{equation*}
%     \vBrace{\text{Cov}_{\theta}\cBrace{\hat{\theta},l^{\prime}\rBrace{\vecnot{X}|\theta}}}^{2}\leq{}\text{Var}_{\theta}\cBrace{\hat{\theta}}\text{Var}_{\theta}\cBrace{l^{\prime}\rBrace{\vecnot{X}|\theta}}=\text{Var}_{\theta}\cBrace{\hat{\theta}}I_{n}\rBrace{\theta}
% \end{equation*}
% i.e.
In practical applications, where signal is noised and parameters cannot be accurately estimated, having a closed form expression for the FIM naturally leads to seek for a performance bound that will help the user to know when available information is fully utilized.
% \begin{equation*}
%     \vBrace{m^{\prime}\rBrace{\theta}}^{2}\leq{}\text{Var}_{\theta}\cBrace{\hat{\theta}}I_{n}\rBrace{\theta}=nI\rBrace{\theta}=\text{Var}_{\theta}{\cBrace{\hat{\theta}}}
% \end{equation*}
% Finally, we get the lower bound of variance of an arbitrary estimator $\hat{\theta}$ as
Such a bound for unbiased estimators is \cite{van2004detection}
% \begin{equation}
% \label{eq_prlm_CramerRao_lim}
%     \text{Var}_{\theta}{\cBrace{\hat{\theta}}}\geq{}\frac{\vBrace{m^{\prime}\rBrace{\theta}}^{2}}{nI\rBrace{\theta}}
% \end{equation}
% where 
% The inequality \eqref{eq_prlm_CramerRao_lim} is called the \emph{information inequality}, and also known as the \emph{Cram\'er-Rao inequality} in honor of the Sweden statistician H. Cram\'er Indian statistician C. R. Rao who independently developed this inequality during the 1940s. 
% The information inequality shows that as $I_{\theta}$ increases, the variance of the estimator decreases, therefore, the quality of the estimator increases, that is why the quantity is called “information”.
% \par
% If $\hat{\theta}$ is an unbiased estimator, then $m\rBrace{\theta}=E_{\theta}\rBrace{\hat{\theta}}=\theta$, $m^{\prime}\rBrace{\theta}=1$.
% Hence, by the information inequality, for unbiased estimator $\hat{\theta}$,
\begin{equation*}
    \text{Var}_{\theta}\cBrace{\hat{\theta}}\geq\frac{1}{nI\rBrace{\theta}}.
\end{equation*}
The right hand side is always called the Cram\'er-Rao lower bound (CRLB): under certain conditions, no other unbiased estimator of the parameter $\theta$ based on an i.i.d sample of size $n$ can have a variance smaller than CRLB.
\subsection{The Multiple Parameter Case}
Suppose now there are more than one parameters in the distribution model, that is, the random variable $X\sim{}f\rBrace{x|\vecnot{\theta}}$ with $\vecnot{\theta}=\rBrace{\theta_{0},\dot,\theta_{k-1}}^{T}$.
We denote the log-likelihood function as
\begin{equation*}
    l\rBrace{\vecnot{\theta}}=\log{f\rBrace{x|\vecnot{\theta}}},
\end{equation*}
and its first order derivative with respect to $\vecnot{\theta}$ is a $k$-dimensional vector, which is
\begin{equation*}
    \frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}}=\rBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{0}},\dots,\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{k-1}}}^{T},
\end{equation*}
The second order derivative of $l\rBrace{\vecnot{\theta}}$ with respect to $\vecnot{\theta}$ is a $k\times{}k$ matrix, which is
\begin{equation*}
    \frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}^{2}}=\cBrace{\frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{i}\partial{}\theta_{j}}}_{i,j\in\vBrace{0,\dots,k-1}}.
\end{equation*}
We define the \emph{Fisher information matrix} as
\begin{equation*}
    I\rBrace{\vecnot{\theta}}=E\cBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}\rBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}}^{T}}=\text{Cov}\cBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}}=-E\cBrace{\frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}^{2}}}.
\end{equation*}
Since the covariance matrix is symmetric and semi-positive definite, these properties hold for the Fisher information matrix as well.
\subsection{FIM applications}
Considering unbiased estimators, the CRLB for the multi-parameter case can be shown to be
\begin{equation}
\label{eq_prlm_multiVar_CRLB}
    \text{Cov}_{\vecnot{\theta}}\cBrace{\hat{\vecnot{\theta}}\rBrace{\vecnot{X}}}\geq{}I^{-1}\rBrace{\vecnot{\theta}}
\end{equation}
where the matrix inequality $A\geq{}B$ means that $A-B$ is positive semi-definite.
From \eqref{eq_prlm_multiVar_CRLB}, it is obvious that when $I$'th determinant is increased, the CRLB decreases which implicates that the measuring is more informative and though not necessarily obtainable, an optimal estimator will achieve higher accuracy in such scenario.
\subsection{Frequency Domain Cram\'er-Rao Bound for Gaussian Processes}
The interesting work of \cite{zeira1990frequency} (which also specifically states its contribution to localization applications) enables the generalization of the CRLB to the frequency domain, which is of high relevance to this work (see Sec.\ref{sec_FIM}).
Assuming Gaussian processes, the results of \cite{whittle1953analysis} are utilized together with further development to find that 
\begin{equation}\label{eq_FIM_kl_full_REAL}
    %\resizebox{.9\linewidth}{!}
    {
        \begin{split}
            J_{k,l}\rBrace{\vEta} 
            =&
            %\Re\cBrace{
            \frac{1}{2\pi}
            \int_{-\omega_{s}/2}^{\omega_{s}/2}
            {
            \frac{1}{\Phi\rBrace{\omega}}
            \mathfrak{F}^{*}\left\{
            \frac{\partial z(t)}{\partial\eta_{k}}
            \right\}
            \mathfrak{F}\left\{
            \frac{\partial z(t)}{\partial\eta_{l}}
            \right\}
            d\omega
            }%}
            \\ &+
            \frac{T}{4\pi}
            \int_{-\omega_{s}/2}^{\omega_{s}/2}
            \frac{1}{\Phi^{2}\rBrace{\omega}}
            \rBrace{\frac{\partial\Phi\rBrace{\omega}}{\partial\eta_{k}}}^{\ast}
            \frac{\partial\Phi\rBrace{\omega}}{\partial\eta_{l}}
            d\omega
        \end{split}
    }
\end{equation}
which is conveniently generalized to the complex data case used in this work as \eqref{eq_FIM_kl_full}.