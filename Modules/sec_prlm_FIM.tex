In the parameter estimation problems, we obtain information about the parameter from asample of data coming from the underlying probability distribution. 
A natural question is:how much information can a sample of data provide about the unknown parameter? 
This section introduces such a measure for information, and we can also see that this informationmeasure can be used to find bounds on the variance of estimators, and it can be used toapproximate the sampling distribution of an estimator obtained from a large sample, andfurther be used to obtain an approximate confidence interval in case of large sample.
In this section, we consider a random variable $X$ for which the pdf or pmf is $f\rBrace{x|\theta}$, wehre $\theta$ is an unknown parameter and $\theta\in\Theta$, with $\Theta$ is the parameter space.
Intuitively, if an event has small probability, then the occurrence of this eventbrings us much information. 
For a random variable $X \sim f\rBrace{x|\theta}$, if $\theta$ were the true value ofthe parameter, the likelihood function should take a big value, or equivalently, the derivativelog-likelihood function should be close to zero, and this is the basic principle of maximumlikelihood estimation.
We define $l\rBrace{x|\theta} = \log{f\rBrace{x|\theta}}$ as the log-likelihood function, and
\begin{equation*}
    l^{\prime}\rBrace{x|\theta} = \frac{\partial}{\partial\theta}\log{f\rBrace{x|\theta}}=\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}
\end{equation*}
where $f^{\prime}\rBrace{x|\theta}$ is the derivative of $f\rBrace{x|\theta}$ with respect to $\theta$.
Similarly, we denote the second order derivative of $f\rBrace{x|\theta}$ with respect to $\theta$ as $f^{\prime\prime}\rBrace{x|\theta}$.
According to the above analysis, if $l^{\prime}\rBrace{X|\theta}$ is close to zero, then it is expected, thus the random variable does not provide much information about $\theta$;
on the other hand, if $\abs{l^{\prime}\rBrace{X|\theta}}$ or $\abs{l^{\prime}\rBrace{X|\theta}}^{2}$ is large, the random variable provides much information about $\theta$.
Thus, we can use $\abs{l^{\prime}\rBrace{X|\theta}}^{2}$ to measure the amount of information provided by $X$.
however, since $X$ is a random variable, we should consider the average case. Thus, we introduce the following definition:
\begin{definition}
\emph{Fisher information} (for $\theta$) contained in the random variable $X$ is defined as:
\begin{equation}
\label{eq_prlm_FI_def}
    I\rBrace{\theta}=E_{\theta}\cBrace{\abs{l^{\prime}\rBrace{X|\theta}}^{2}}=\int{}\abs{l^{\prime}\rBrace{X|\theta}}^{2}f\rBrace{x|\theta}dx.
\end{equation}
\end{definition}
We assume that we can exchange the order of differentiation and integration, then
\begin{equation*}
    \int{}f^{\prime}\rBrace{x|\theta}dx=\frac{\partial}{\partial\theta}\int{}f\rBrace{x|\theta}dx=0.
\end{equation*}
Similarly,
\begin{equation*}
    \int{}f^{\prime\prime}\rBrace{x|\theta}dx=\frac{\partial^{2}}{\partial\theta^{2}}\int{}f\rBrace{x|\theta}dx=0.
\end{equation*}
It is easy to see that
\begin{equation*}
    E_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}=\int{}l^{\prime}\rBrace{x|\theta}f\rBrace{x|\theta}dx=\int{}\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}f\rBrace{x|\theta}dx=\int{}f^{\prime}\rBrace{x|\theta}=0
\end{equation*}
Therefore, the definition of Fisher information \eqref{eq_prlm_FI_def} can be rewritten as
\begin{equation}
\label{eq_prlm_IeqVar}
    I\rBrace{\theta}=\text{Var}_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}
\end{equation}.
Also, notice that
\begin{equation*}
    l^{\prime\prime}\rBrace{x|\theta}=\frac{\partial}{\partial\theta}\vBrace{\frac{f^{\prime}\rBrace{x|\theta}}{f\rBrace{x|\theta}}}=\frac{f^{\prime\prime}\rBrace{x|\theta}f\rBrace{x|\theta}-\abs{f^{\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}=\frac{\abs{f^{\prime\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}-\abs{l^{\prime}\rBrace{x|\theta}}^{2}.
\end{equation*}
Therefore,
\begin{equation*}
    E_{\theta}\cBrace{l^{\prime\prime}\rBrace{x|\theta}}=\int{\vBrace{\frac{\abs{f^{\prime\prime}\rBrace{x|\theta}}^{2}}{\abs{f\rBrace{x|\theta}}^{2}}-\abs{l^{\prime}\rBrace{x|\theta}}^{2}}f\rBrace{x|\theta}}=\int{f^{\prime\prime}\rBrace{x|\theta}dx}-E_{\theta}\cBrace{l^{\prime}\rBrace{X|\theta}}=-I\rBrace{\theta}.
\end{equation*}
Finally, we have another formula to calculate Fisher information:
\begin{equation}
\label{eq_prlm_Ifin}
    I\rBrace{\theta} = -E_{\theta}\cBrace{l^{\prime\prime}\rBrace{x|\theta}}=-\int{\vBrace{\frac{\partial^{2}}{\partial\theta^{2}}\log{f\rBrace{x|\theta}}f\rBrace{x|\theta}}dx}
\end{equation}.
To summarize, we have three methods to calculate Fisher information: equations \eqref{eq_prlm_FI_def}, \eqref{eq_prlm_IeqVar} and \eqref{eq_prlm_Ifin}.
In any problems, using \eqref{eq_prlm_Ifin} is the most convenient choice.
\subsection{Cram\'er-Rao Lower Bound and Asymptotic Distribution of Maximum Likelihood Estimators}
Suppose that we have a random sample $X_{1},\dots,X_{n}$ coming from a distribution for which the pdf or pmf is $f\rBrace{x|\theta},$ where the value of the parameter $\theta$ is unknown. 
We will show how to used Fisher information to determine the lower bound for the variance of an estimator of the parameter $\theta$.
\par
Let $\hat{\theta}=r\rBrace{X_{1},\dots,X_{n}}=r\rBrace{\vecnot{X}}$ be an arbitrary estimator of $\theta$.
Assume $E_{\theta}\rBrace{\hat{\theta}}=m\rBrace{\theta}$ and the variance of $\hat{\theta}$ is finite.
Let us consider the random variable $l^{\prime}\rBrace{\vecnot{X}|\theta}$, it can be shown that $E_{\theta}\cBrace{l^{\prime}\rBrace{\vecnot{X}|\theta}}=0$ and that
\begin{equation}
\label{eq_prlm_covThetaLTag}
    \text{Cov}_{\theta}\cBrace{\hat{\theta},l^{\prime}\rBrace{\vecnot{X}|\theta}}=m^{\prime}\rBrace{\theta}.
\end{equation}.
By Cauchy-Schwartz inequality and the definition of $I_{n}\rBrace{\theta},$
\begin{equation*}
    \vBrace{\text{Cov}_{\theta}\cBrace{\hat{\theta},l^{\prime}\rBrace{\vecnot{X}|\theta}}}^{2}\leq{}\text{Var}_{\theta}\cBrace{\hat{\theta}}\text{Var}_{\theta}\cBrace{l^{\prime}\rBrace{\vecnot{X}|\theta}}=\text{Var}_{\theta}\cBrace{\hat{\theta}}I_{n}\rBrace{\theta}
\end{equation*}
i.e.
\begin{equation*}
    \vBrace{m^{\prime}\rBrace{\theta}}^{2}\leq{}\text{Var}_{\theta}\cBrace{\hat{\theta}}I_{n}\rBrace{\theta}=nI\rBrace{\theta}=\text{Var}_{\theta}{\cBrace{\hat{\theta}}}
\end{equation*}
Finally, we get the lower bound of variance of an arbitrary estimator $\hat{\theta}$ as
\begin{equation}
\label{eq_prlm_CramerRao_lim}
    \text{Var}_{\theta}{\cBrace{\hat{\theta}}}\geq{}\frac{\vBrace{m^{\prime}\rBrace{\theta}}^{2}}{nI\rBrace{\theta}}
\end{equation}
The inequality \eqref{eq_prlm_CramerRao_lim} is called the \emph{information inequality}, and also known as the \emph{Cram\'er-Rao inequality} in honor of the Sweden statistician H. Cram\'er Indian statistician C. R. Rao who independently developed this inequality during the 1940s. 
The information inequality shows that as $I_{\theta}$ increases, the variance of the estimator decreases, therefore, the quality of the estimator increases, that is why the quantity is called “information”.
\par
If $\hat{\theta}$ is an unbiased estimator, then $m\rBrace{\theta}=E_{\theta}\rBrace{\hat{\theta}}=\theta$, $m^{\prime}\rBrace{\theta}=1$.
Hence, by the information inequality, for unbiased estimator $\hat{\theta}$,
\begin{equation*}
    \text{Var}_{\theta}\cBrace{\hat{\theta}}\geq\frac{1}{nI\rBrace{\theta}}.
\end{equation*}
The right hand side is always called the Cram\'er-Rao lower bound (CRLB): under certain conditions, no other unbiased estimator of the parameter $\theta$ based on an i.i.d sample of size $n$ can have a variance smaller than CRLB.
\subsection{The Multiple Parameter Case}
Suppose now there are more than one parameters in the distribution model, that is, the random variable $X\sim{}f\rBrace{x|\vecnot{\theta}}$ with $\vecnot{\theta}=\rBrace{\theta_{0},\dot,\theta_{k-1}}^{T}$.
We denote the log-likelihood function as
\begin{equation*}
    l\rBrace{\vecnot{\theta}}=\log{f\rBrace{x|\vecnot{\theta}}},
\end{equation*}
and its first order derivative with respect to $\vecnot{\theta}$ is a $k$-dimensional vector, which is
\begin{equation*}
    \frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}}=\rBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{0}},\dots,\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{k-1}}}^{T},
\end{equation*}
The second order derivative of $l\rBrace{\vecnot{\theta}}$ with respect to $\vecnot{\theta}$ is a $k\times{}k$ matrix, which is
\begin{equation*}
    \frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}^{2}}=\cBrace{\frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\theta_{i}\partial{}\theta_{j}}}_{i,j\in\vBrace{0,\dots,k-1}}.
\end{equation*}
We define the \emph{Fisher information matrix} as
\begin{equation*}
    I\rBrace{\vecnot{\theta}}=E\cBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}\rBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}}^{T}}=\text{Cov}\cBrace{\frac{\partial{}l\rBrace{\vecnot{\theta}}}{\partial\vecnot{\theta}}}=-E\cBrace{\frac{\partial^{2}l\rBrace{\vecnot{\theta}}}{\partial{}\vecnot{\theta}^{2}}}.
\end{equation*}
Since the covariance matrix is symmetric and semi-positive definite, these properties hold for the Fisher information matrix as well.
\subsection{FIM applications}
Considering unbiased estimators, the CRLB for the multi-parameter case can be shown to be
\begin{equation}
\label{eq_prlm_multiVar_CRLB}
    \test{Cov}_{\vecnot{\theta}}\cBrace{\hat{\vecnot{\theta}}\rBrace{\vecnot{X}}}\geq{}I^{-1}\rBrace{\vecnot{\theta}}
\end{equation}
where the matrix inequality $A\geq{}B$ means that $A-B$ is positive semi-definite.
From \eqref{eq_prlm_multiVar_CRLB}, it is obvious that when $I$'th determinant is increased, the CRLB decreases which implicates that the measuring is more informative and though not necessarily obtainable, an optimal estimator will achieve higher accuracy in such scenario.